{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Mining\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore more advanced machine learning techniques with text data. First, we introduce the concept of n-grams, which are combinations of one or more tokens. For example, bigrams are combinations of two tokens, while trigrams are combinations of three tokens. Next, we introduce the concept of stemming, which is used to convert tokens into their root forms so that token frequencies match the use of the root token rather than being spread across multiple similar tokens. Finally, we explore the application of clustering and feature selection to text data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[N-Grams](#N-Grams)\n",
    "\n",
    "[N-Gram Classification](#N-Gram-Classification)\n",
    "\n",
    "[Stemming](#Stemming)\n",
    "\n",
    "[Clustering Analysis](#Clustering-Analysis)\n",
    "\n",
    "[Dimension-Reduction](#Dimension-Reduction)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the rest of this notebook, we first include the notebook setup code and we define our _home_ directory.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we find our HOME directory\n",
    "home_dir = !echo $HOME\n",
    "\n",
    "# Then we create a temporary working directory to store data files in\n",
    "!mkdir ~/temp_w9l3\n",
    "\n",
    "# We construct the full path, below our   \n",
    "# HOME directory to the temporary working directory location\n",
    "home=home_dir[0] + '/temp_w9l3/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## N-Grams\n",
    "\n",
    "Formally, a [_n-gram_][ngd] is a contiguous sequence of **n** items from a parent sequence of items, such as characters or words in a text document. In general, we will focus solely on words in a document. Thus, our initial approach has simply been to look at unigrams or single words in a document when building a classification model. However, sometimes the combination of words can be more descriptive, for example, _unbelievably bad_ is generally viewed as a more powerful description than just _bad_. As a result, the concept of an _n-gram_ was created, where collections of words can be treated as features. In fact google allows a user to search for [specific n-gram][gnv] combinations in books that they have scanned.\n",
    "\n",
    "While this clearly can improve classification power, it also increases computational requirements. This is a result of the exponential rise in the number of possible features. For example, given $n$ words, we have $n \\times (n - 1)$ possible bigrams, and so on for higher order combinations. While this is not a problem for small vocabularies, for larger vocabularies (and corresponding documents) the number of possible features can quickly become very large. Thus, many text mining applications will make use of Hadoop or Spark clusters to leverage the inherent parallelism in these tasks.\n",
    "\n",
    "To demonstrate using n-grams, we first demonstrate the concept on a single sentence.\n",
    "\n",
    "-----\n",
    "[gnv]: https://books.google.com/ngrams\n",
    "[ngd]: https://en.wikipedia.org/wiki/N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'this', 'course', 'introduces', 'many', 'concepts', 'in', 'data', 'science',\n",
      "  'this course', 'course introduces', 'introduces many', 'many concepts',\n",
      "  'concepts in', 'in data', 'data science', 'this course introduces',\n",
      "  'course introduces many', 'introduces many concepts', 'many concepts in',\n",
      "  'concepts in data', 'in data science']\n"
     ]
    }
   ],
   "source": [
    "my_text = 'This course introduces many concepts in data science.'\n",
    "\n",
    "# Tokenize sentance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "# Analyze sentance\n",
    "tk_func = cv.build_analyzer()\n",
    "\n",
    "# Display n-grams\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, \n",
    "                          width=80, compact=True)\n",
    "pp.pprint(tk_func(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can create a new document-term matrix based on this sentence, and use this matrix (or vector since it is only one row), to provide a representation space for new sentences. In the following Code cell, we tokenize our original sentence, and sort the resulting vocabulary (i.e., n-grams) with their ranking (or order). Next, we create a second, simple sentence and compute the indices into the original DTM for the n-grams in the new sentence. The result is display as a bit vector, where `1` means the corresponding n-gram is in the new sentence, and `0` means it is not.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token mapping:\n",
      "----------------------------------------\n",
      "0 concepts\n",
      "1 concepts in\n",
      "2 concepts in data\n",
      "3 course\n",
      "4 course introduces\n",
      "5 course introduces many\n",
      "6 data\n",
      "7 data science\n",
      "8 in\n",
      "9 in data\n",
      "10 in data science\n",
      "11 introduces\n",
      "12 introduces many\n",
      "13 introduces many concepts\n",
      "14 many\n",
      "15 many concepts\n",
      "16 many concepts in\n",
      "17 science\n",
      "18 this\n",
      "19 this course\n",
      "20 this course introduces\n",
      "----------------------------------------\n",
      "['This course is data science!']\n",
      "----------------------------------------\n",
      "[[0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Display token mapping\n",
    "in_list = []\n",
    "in_list.append(my_text)\n",
    "\n",
    "# Tokenize sentence\n",
    "cv = cv.fit(in_list)\n",
    "\n",
    "# Sort tokens\n",
    "import operator\n",
    "my_voc = sorted(cv.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "\n",
    "# Display token mapping\n",
    "print('Token mapping:')\n",
    "print(40*'-')\n",
    "\n",
    "for tokens, rank in my_voc:\n",
    "    print(rank, tokens)\n",
    "\n",
    "# Display new sentence\n",
    "print(40*'-')\n",
    "out_list = ['This course is data science!']\n",
    "\n",
    "# Transform new sentence to original sentence DTM\n",
    "xsm = cv.transform(out_list)\n",
    "print(out_list)\n",
    "\n",
    "# Display count vector indices for new sentance tokens\n",
    "print(40*'-')\n",
    "print(xsm.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we used `CountVectorizer` to create n-gram tokens. Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the `CountVectorizer` to use stop words and change the tokens to all lowercase. How does this change the tokens and mappings? \n",
    "2. Create your own sentence, do the tokens and n-grams make sense?\n",
    "3. Try changing the ngram range to different values (e.g., `ngram_range=(1,4)` or `ngram_range=(2,3)`). Notice how the number of tokens quickly increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## N-Gram Classification\n",
    "\n",
    "Having n-grams offers improved classification, since word or token combinations often include more information than single words or tokens. For example, _University Illinois_ means more than just _University_ and _Illinois_. We can build on our previous simple text classification pipeline to now develop a more complete code example that builds a feature vector containing both single words and b-grams from the documents. We use this new sparse matrix to classify the documents by using our simple Naive Bayes classifier, which obtains slightly better results. First, we load the movie review data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews: 2000\n"
     ]
    }
   ],
   "source": [
    "# Load movie corpus\n",
    "import nltk\n",
    "mvr = nltk.corpus.movie_reviews\n",
    "\n",
    "# Use scikit learn to split into training and testing\n",
    "from sklearn.datasets import load_files\n",
    "data_dir = '/home/data_scientist/data/nltk_data/corpora/movie_reviews'\n",
    "\n",
    "mvr = load_files(data_dir, shuffle = False)\n",
    "print('Number of Reviews: {0}'.format(len(mvr.data)))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "mvr_train, mvr_test, y_train, y_test = train_test_split(\n",
    "    mvr.data, mvr.target, test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.71      0.77       259\n",
      "        pos       0.73      0.85      0.79       241\n",
      "\n",
      "avg / total       0.79      0.78      0.78       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes pipeline to classify\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "pclf = Pipeline(tools)\n",
    "\n",
    "# Lowercase and restrict ourselves to about \n",
    "# half the available features\n",
    "pclf.set_params(cv__stop_words = 'english',\n",
    "                cv__ngram_range=(1,2),\n",
    "                cv__lowercase=True)\n",
    "\n",
    "# Fit model, predict, and display results\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 420997\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "\n",
    "# Display number of features\n",
    "print(f'Number of Features = {clf.feature_log_prob_.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Now we can repeat the results, but now use unigrams, bigrams, and trigrams. Since this will produce a document term matrix that likely exceeds the computational resources of our Docker container (i.e., a very large DTM can crash your server), we impose two cuts on the tokens included in our DTM. First, we impose a minimum feature term that requires a term to be present in at least two documents. Second, we set a maximum frequency of 50%, such that any term occurring in more than fifty percent of all documents will be ignored (these are likely all stop words, but this value can be reduced to a lower value to trim frequently occurring words that add little to the accuracy. Notice that our overall number of features is much smaller, even though we are also using trigrams in this classification pipeline.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.78      0.81       259\n",
      "        pos       0.78      0.83      0.81       241\n",
      "\n",
      "avg / total       0.81      0.81      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Restrict vocabulary, even with trigrams\n",
    "pclf.set_params(cv__stop_words = 'english',\n",
    "                cv__ngram_range=(1,3),\n",
    "                cv__lowercase=True,\n",
    "                cv__min_df=2,\n",
    "                cv__max_df=0.5)\n",
    "\n",
    "# Fit, predict, and display results\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 62734\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "\n",
    "# Display number of features\n",
    "print(f'Number of Features = {clf.feature_log_prob_.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we included n-grams in the classification process by using a `CountVectorizer`.  Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Try to determine a value of `max_df` that replicates the effects of using stop words. \n",
    "2. Change `CountVectorizer` to `TfidfVectorizer`. Do the results change?\n",
    "3. Try using a more powerful classification algorithm, as opposed to `MultinomialNB`. Do the results change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Stemming\n",
    "\n",
    "So far, we have looked at several techniques to remove redundant or unimportant features. For example, we changed the case of all text to lowercase and we have applied stop words. However, there still is the issue of different forms of the same word, for example compute, computer, computed, and computing. The process of changing words back to their root, or basic form (by removing prefixes and suffixes) is known as stemming. \n",
    "\n",
    "The most widely used stemmer, or program/method that performs stemming, is the _Porter Stemmer_, which was originally published in 1980 by Martin Porter. An improved version was released in 2000, which fixed a number of errors. NLTK includes the Porter Stemmer, which can be used with scikit learn by creating a special function that tokenizes text documents and passing this function as an argument to the `CountVectorizer` via the `tokenizer` attribute. By performing stemming inside this tokenize method, we can return a set of tokens for a document that have been stemmed. In the following Code cell, we use a custom `tokenize` method that first builds a list of tokens by using NLTK, and then maps the Porter stemmer to the list of tokens to generate a stemmed list.\n",
    "\n",
    "-----\n",
    "[ws]: https://en.wikipedia.org/wiki/Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.83      0.77      0.80       259\n",
      "        pos       0.77      0.83      0.80       241\n",
      "\n",
      "avg / total       0.80      0.80      0.80       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Define function to tokenize text and apply stemmer\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = map(stemmer.stem, tokens)\n",
    "    return stems\n",
    "\n",
    "# Define vocabulary, including stemming from our function\n",
    "pclf.set_params(cv__stop_words = 'english',\n",
    "                cv__ngram_range=(1,3),\n",
    "                cv__lowercase=True,\n",
    "                cv__tokenizer=tokenize)\n",
    "\n",
    "# Fit, predict, and display results\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test[0:500])\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 80529\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "\n",
    "# Display number of features\n",
    "print(f'Number of Features = {clf.feature_log_prob_.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we incorporated the Porter Stemmer into the classification pipeline. Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Did the Porter Stemmer improve the classification results (note you need to be sure you are comparing exactly the same pipeline (including the use of ngrams)? \n",
    "\n",
    "2. Can you compute the number of features that the Porter Stemmer generates? How does this compare the number of features without stemming?\n",
    "\n",
    "3. Try using a different stemming algorithm, such as [_snowball_][nsw]. How do the classification results change? \n",
    "\n",
    "-----\n",
    "[nsw]: http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Clustering Analysis\n",
    "\n",
    "We can also apply clustering analysis to our feature matrix. While finding an unknown number of clusters in text documents can be difficult, we can learn about our data by identifying the clusters for our **known** labels. To demonstrate, in the following Code cells, we employ k-means to find two clusters in our feature matrix (the movie reviews), after which we identify the most frequently used words in each cluster.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=2, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply k-means clustering to movie reviews\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Pos/Neg clusters\n",
    "true_k = 2\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', \n",
    "            max_iter=100, n_init=1)\n",
    "\n",
    "# Verify attributes\n",
    "cv = CountVectorizer(stop_words = 'english', \\\n",
    "                     ngram_range=(1, 3), \n",
    "                     max_features=100000)\n",
    "\n",
    "# Transform vocabulary\n",
    "train_counts = cv.fit_transform(mvr_train)\n",
    "test_data = cv.transform(mvr_test)\n",
    "\n",
    "# Determine clusters\n",
    "km.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens per cluster:\n",
      "\n",
      "Cluster 0: joe mickey mallory cowboy kill new dream way new york york american like life just scene killers american dream man place clyde\n",
      "\n",
      "Cluster 1: film movie like just time good story character way characters make does plot really scene people life bad little man\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract top tokens\n",
    "top_tokens = 20\n",
    "labels = ['Neg', 'Pos']\n",
    "\n",
    "print(f'Top {top_tokens} tokens per cluster:\\n')\n",
    "\n",
    "# Compute cluster centers\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "# Extract tokens\n",
    "terms = cv.get_feature_names()\n",
    "\n",
    "# Display top tokens per cluster\n",
    "for idx in range(true_k):\n",
    "    print(f'Cluster {idx}:', end='')\n",
    "    for jdx in order_centroids[idx, :top_tokens]:\n",
    "        print(f' {terms[jdx]}', end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can perform the same analysis on a more complex problem by analyzing the twenty newsgroup data set. First we load the data, after which we apply k-means and identify the most common tokens in each cluster.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load training and testing samples\n",
    "train = fetch_20newsgroups(data_home=home + 'textdm', \n",
    "                           subset='train', shuffle=True, random_state=23)\n",
    "test = fetch_20newsgroups(data_home=home + 'textdm', \n",
    "                          subset='test', shuffle=True, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=20, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cluster twenty newsgroups\n",
    "true_k = 20\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "# Verify attributes\n",
    "cv = CountVectorizer(stop_words = 'english', \n",
    "                     max_features=100000)\n",
    "\n",
    "# Transform words\n",
    "train_counts = cv.fit_transform(train['data'])\n",
    "test_data = cv.transform(test['data'])\n",
    "\n",
    "# Fit clusters\n",
    "km.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens per cluster:\n",
      "\n",
      "Cluster 0: myers ms president think don dee ll know said going decision does house white today believe justice just board department\n",
      "\n",
      "Cluster 1: jehovah elohim lord god christ father mcconkie unto son ps jesus said gods shall thou thee mormon thy earth stated\n",
      "\n",
      "Cluster 2: planet earth spacecraft solar surface sun venus moon atmosphere planets jupiter kilometers miles years space degrees moons mars saturn mariner\n",
      "\n",
      "Cluster 3: edu com cancer use file health ftp server disease 1993 information hiv medical number new 11 venus rx xfree86 available\n",
      "\n",
      "Cluster 4: mb m4 ms mz ma mm mo m1 mu mc mh mw mp mt mj mk mx mf mn m3\n",
      "\n",
      "Cluster 5: people god don just like think know say does edu way time right said did believe government new good make\n",
      "\n",
      "Cluster 6: president mr stephanopoulos think know did general going people just don said attorney believe say decision question make time good\n",
      "\n",
      "Cluster 7: edu writes subject article lines organization don university just com like people think know posting host nntp time cs good\n",
      "\n",
      "Cluster 8: dos windows microsoft tcp ms mouse amiga software pc graphics higher network macintosh mbytes version 00 ip memory support os\n",
      "\n",
      "Cluster 9: god psalms christ lord prayers jesus kingdom church people man enemies wicked earth 10 judgement praying word day pray words\n",
      "\n",
      "Cluster 10: jpeg image gif file color format images quality version files bit free programs available use jfif software edu display don\n",
      "\n",
      "Cluster 11: said mac door files started went don comp disk didn children people armenians file time ll software sys know macintosh\n",
      "\n",
      "Cluster 12: 00 20 appears 40 art 50 80 10 wolverine 60 1st ghost rider hobgoblin man annual punisher appear sabretooth cover\n",
      "\n",
      "Cluster 13: edu graphics pub mail ray 128 send 3d ftp com server objects amiga rayshade archie image images files file archive\n",
      "\n",
      "Cluster 14: openwindows use sun xview usr look x11 lib open subject file openwinhome window olit news programs openwin manual fonts bin\n",
      "\n",
      "Cluster 15: image edu data available graphics ftp software pub processing images package format analysis display formats information file version program files\n",
      "\n",
      "Cluster 16: 92 12 10 hiv 17 11 aids patients et medical 03 30 25 cd4 milk 31 1993 tb 09 number\n",
      "\n",
      "Cluster 17: com subject lines organization edu writes posting article host nntp university like ca know just don does distribution new use\n",
      "\n",
      "Cluster 18: vitamin liver infection serum carotene beta zinc clinical time good level dl acid cancer use reserves important rda chronic going\n",
      "\n",
      "Cluster 19: gopher search edu client database pub software ftp information veronica data macintosh mail world micro available retrieve clients unix sites\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_tokens = 20\n",
    "labels = test['target']\n",
    "\n",
    "print('Top {} tokens per cluster:\\n'.format(top_tokens))\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "\n",
    "for idx in range(true_k):\n",
    "    print(\"Cluster {0}:\".format(idx), end='')\n",
    "    for jdx in order_centroids[idx, :top_tokens]:\n",
    "        print(' {0}'.format(terms[jdx]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "\n",
    "In the preceding cells, we used k-means clustering to find clusters in text data, and to identify the most important tokens in these clusters. Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the vectorizer to use TF-IDF. Does this change the tokens in each cluster? \n",
    "2. Change the vectorizer to use ngrams. Does this change the tokens in each cluster?\n",
    "3. Include stemming in the vectorizer. Does this change the tokens in each cluster?\n",
    "4. Try using DBSCAN instead of k-means. How many clusters are found? What are the tokens in each cluster?\n",
    "\n",
    "Finally, the k-means algorithm finds clusters. Do these clusters map directly into the _real_ categories? Feel free to discuss this on the course forums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Dimension Reduction\n",
    "\n",
    "The document term matrices we have constructed in these examples can become quite large. We have already reduced the number of features used in classification problems by using stop words, by using a consistent case, and by performing stemming. On the other hand, we have enabled exponential increases in the feature space by including n-grams (although we once again restricted the feature space by using the `max_features` or the `max_df` and `min_df` attributes). The traditional dimensional reduction technique we have used in the past is PCA. However, PCA can be difficult to use with text data given the large sizes of the matrices (since a matrix inversion can be required). We can employ alternative techniques, such as incremental PCA or Truncated SVD. \n",
    "\n",
    "But in reality, we are less interested in finding a reduced dimensional space than we are in removing features that contain little or no information (combining features is essentially increasing the ngram range). In this case, the problem of dimension reduction becomes one of optimal feature selection. For this, we can use the scikit learn `SelectKBest` method to identify the best $k$ features. In the following Code cells, we first create a vectorizer, train it on data to demonstrate the accuracy and the number of features required to achieve that accuracy. After which, we employ `SelectKBest` to identify the best number of features, where number is predetermined, to achieve a similar accuracy. In the end, we find that less than ten percent of the original features are required to achieve the same level of accuracy.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following Example was insipred by scikit learn demo\n",
    "# http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html\n",
    "\n",
    "# Apply TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy =  82.0%\n",
      "Number of Features = 129791\n"
     ]
    }
   ],
   "source": [
    "# First, train on normal set of features, baseline performance.\n",
    "train_counts = tf.fit_transform(train['data'])\n",
    "test_data = tf.transform(test['data'])\n",
    "\n",
    "# Naive Bayes classification\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_counts, train['target'])\n",
    "predicted = nb.predict(test_data)\n",
    "\n",
    "# Display results\n",
    "scr = 100.0 * nb.score(test_data, test['target'])\n",
    "print(f'Prediction accuracy = {scr:5.1f}%')\n",
    "print(f'Number of Features = {nb.feature_log_prob_.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now employ feature selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Number of features to keep\n",
    "num_k = 10000\n",
    "\n",
    "# Employ select k best wiht chi2 since this works with sparse matrices.\n",
    "ch2 = SelectKBest(chi2, k=num_k)\n",
    "xtr = ch2.fit_transform(train_counts, train['target'])\n",
    "xt = ch2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy =  82.0%\n",
      "Number of Features = 10000\n"
     ]
    }
   ],
   "source": [
    "# Train simple model and compute accuracy\n",
    "nb = nb.fit(xtr, train['target'])\n",
    "predicted = nb.predict(xt)\n",
    "\n",
    "# Display results\n",
    "scr = 100.0 * nb.score(xt, test['target'])\n",
    "print(f'Prediction accuracy = {scr:5.1f}%')\n",
    "print(f'Number of Features = {nb.feature_log_prob_.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can use the feature selection to identify the top features for each category. First we extract the feature names, after which we extract the importance (or support) for each feature. By sorting the array containing the important features, we can identify the top tokens.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract feature names\n",
    "feature_names = tf.get_feature_names()\n",
    "\n",
    "# Compute most important feature names\n",
    "indices = ch2.get_support(indices=True)\n",
    "fn = np.array([feature_names[idx] for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alt.atheism:\n",
      "['keith', 'god', 'caltech', 'atheists', 'livesey']\n",
      "\n",
      "comp.graphics:\n",
      "['graphics', 'image', 'thanks', '3d', 'files']\n",
      "\n",
      "comp.os.ms-windows.misc:\n",
      "['windows', 'dos', 'file', 'files', 'driver']\n",
      "\n",
      "comp.sys.ibm.pc.hardware:\n",
      "['drive', 'card', 'scsi', 'ide', 'bus']\n",
      "\n",
      "comp.sys.mac.hardware:\n",
      "['mac', 'apple', 'quadra', 'drive', 'centris']\n",
      "\n",
      "comp.windows.x:\n",
      "['window', 'motif', 'mit', 'com', 'server']\n",
      "\n",
      "misc.forsale:\n",
      "['sale', 'offer', 'shipping', 'distribution', 'condition']\n",
      "\n",
      "rec.autos:\n",
      "['car', 'cars', 'com', 'article', 'engine']\n",
      "\n",
      "rec.motorcycles:\n",
      "['bike', 'dod', 'com', 'ride', 'motorcycle']\n",
      "\n",
      "rec.sport.baseball:\n",
      "['baseball', 'year', 'game', 'team', 'players']\n",
      "\n",
      "rec.sport.hockey:\n",
      "['hockey', 'team', 'game', 'ca', 'nhl']\n",
      "\n",
      "sci.crypt:\n",
      "['clipper', 'key', 'encryption', 'chip', 'com']\n",
      "\n",
      "sci.electronics:\n",
      "['com', 'use', 'circuit', 'host', 'power']\n",
      "\n",
      "sci.med:\n",
      "['pitt', 'geb', 'banks', 'gordon', 'com']\n",
      "\n",
      "sci.space:\n",
      "['space', 'nasa', 'moon', 'orbit', 'henry']\n",
      "\n",
      "soc.religion.christian:\n",
      "['god', 'jesus', 'christians', 'rutgers', 'christian']\n",
      "\n",
      "talk.politics.guns:\n",
      "['gun', 'guns', 'com', 'people', 'batf']\n",
      "\n",
      "talk.politics.mideast:\n",
      "['israel', 'israeli', 'jews', 'turkish', 'arab']\n",
      "\n",
      "talk.politics.misc:\n",
      "['com', 'cramer', 'article', 'people', 'clinton']\n",
      "\n",
      "talk.religion.misc:\n",
      "['god', 'sandvik', 'com', 'christian', 'jesus']\n"
     ]
    }
   ],
   "source": [
    "# Display top 5 important feature names\n",
    "top_count = 5\n",
    "\n",
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_names = np.argsort(nb.coef_[idx])[-top_count:]\n",
    "    tn_lst = [name for name in fn[top_names]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print(f'\\n{target}:')\n",
    "    pp.pprint(tn_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean up - removing the temporary directory we created at the beginning of the lesson\n",
    "!rm -rf $HOME/temp_w9l3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we used feature selection to identify the most important features in our simple classification pipeline. Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the vectorizer to change the case of all words an to employ stemming. How do the results (tokens) change?\n",
    "\n",
    "2. Change the classification algorithm to a more accurate method. How do the results change? How does the computational time change?\n",
    "\n",
    "Finally, what do the list of tokens say about the fact we did not remove headers or footers from the newsgroup postings? Feel free to comment on these questions in the course forum.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. Wikipedia articles on [n-grams][wng], [Stemming][wst], and [Lemmatization][wl]\n",
    "1. Google [n-gram viewer][gnv]\n",
    "1. Alternative [n-gram viewer][anv]\n",
    "1. Wikipedia article on [Text Clustering][wtc]\n",
    "1. Blog on Sentiment Analysis with NLTK, [Part III][bsa3] and [Part IV][bsa4]\n",
    "1. An online [Stemming Demo][std] using NLTK\n",
    "1. A [treatise on Snowball][tsb] discussing, in depth, the process of stemming.\n",
    "1. A discourse on [Dimension Reduction][msdr]\n",
    "1. [Local Linear Embedding][lle], for dimensional reduction\n",
    "1. [Independent Component Analysis][ica] for text data\n",
    "\n",
    "-----\n",
    "\n",
    "[wst]: https://en.wikipedia.org/wiki/Stemming\n",
    "[wl]: https://en.wikipedia.org/wiki/Lemmatisation\n",
    "[wtc]: https://en.wikipedia.org/wiki/Document_clustering\n",
    "\n",
    "[tsb]: http://snowball.tartarus.org/texts/introduction.html\n",
    "[std]: http://text-processing.com/demo/stem/\n",
    "\n",
    "[wng]: https://en.wikipedia.org/wiki/N-gram\n",
    "\n",
    "[gnv]: https://books.google.com/ngrams\n",
    "[anv]: http://xkcd.culturomics.org\n",
    "\n",
    "[bsa3]: http://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/\n",
    "[bsa4]: http://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/\n",
    "\n",
    "[msdr]: http://research.microsoft.com/pubs/150728/FnT_dimensionReduction.pdf\n",
    "[lle]: http://science.sciencemag.org/content/290/5500/2323.abstract\n",
    "[ica]: http://www.cs.rutgers.edu/~mlittman/topics/dimred02/kolenda99independent.pdf1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2017: Robert J. Brunner at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
